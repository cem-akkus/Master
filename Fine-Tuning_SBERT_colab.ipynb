{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling of Asset Returns: Fine-Tuning_SBERT_colab\n",
    "\n",
    "**Author:** Cem Akkus  \n",
    "**Institution:** Ludwig-Maximilians-Universität München  \n",
    "**Date:** 14.07.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Important note: This code was fully executed on Google Colab__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Pre-Processing](#pre)  \n",
    "    1.1. [Modules & Seeds](#module)  \n",
    "    1.2. [Data Import](#loading)  \n",
    "    1.3. [Data Transformations](#data-transform)    \n",
    "2. [Fine-Tuning](#fine-tuning)  \n",
    "    2.1. [Definitions](#definitions)  \n",
    "    2.2. [Fine-Tuning Process](#fine-tuning-process)  \n",
    "3. [Post-Processing](#post)  \n",
    "    3.1. [Embeddings Generation](#emb-gen)  \n",
    "    3.2. [Embeddings Averaging over Dates](#emm-avg)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-Processing\n",
    "<a name=\"pre\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Modules & Seeds\n",
    "<a name=\"module\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# specify GPU\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "# Set a random seed for NumPy\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set a random seed for Python's built-in random module\n",
    "random.seed(seed)\n",
    "\n",
    "# Set a random seed for PyTorch (for GPU and CPU)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Set a seed for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Data Import\n",
    "<a name=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_path = '/content/drive/My Drive/Colab Notebooks/'\n",
    "file_list = os.listdir(drive_path)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the CSV file\n",
    "file_path = '/content/drive/My Drive/Colab Notebooks/headlines_adapted_SAP_DE.csv'  # Adjust the path for every company\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "dataset = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data Transformations\n",
    "<a name=\"data-transform\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['1d_return_movement'] = np.where(dataset['1d_return'] > 0, 'Up', 'Down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of 0 and 1 in the '1d_return_movement' column of the DataFrame\n",
    "movement_counts = dataset['1d_return_movement'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(movement_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train dataset into train, validation and test sets\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['title'], dataset['1d_return_movement'],\n",
    "                                                                    random_state=1,\n",
    "                                                                    test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to binary arrays for multi-label classification\n",
    "label_binarizer = MultiLabelBinarizer()\n",
    "train_labels = label_binarizer.fit_transform(train_labels)\n",
    "temp_labels = label_binarizer.transform(temp_labels)\n",
    "\n",
    "# Convert the binary labels to a single column of class indices\n",
    "train_labels = np.argmax(train_labels, axis=1)\n",
    "temp_labels = np.argmax(temp_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\n",
    "                                                                random_state=1,\n",
    "                                                                test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_text.tolist()\n",
    "val_text = val_text.tolist()\n",
    "test_text = test_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_embeddings = sbert.encode(train_text, convert_to_tensor=True)\n",
    "val_text_embeddings = sbert.encode(val_text, convert_to_tensor=True)\n",
    "test_text_embeddings = sbert.encode(test_text, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors (now sentence embeddings) and labels in TensorDataset\n",
    "train_data = TensorDataset(train_text_embeddings, torch.tensor(train_labels))\n",
    "val_data = TensorDataset(val_text_embeddings, torch.tensor(val_labels))\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-Tuning\n",
    "<a name=\"fine-tuning\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Definitions\n",
    "<a name=\"definitions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "\n",
    "      super(SBERT_Arch, self).__init__()\n",
    "\n",
    "      self.sbert = sbert\n",
    "\n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(384,384)\n",
    "\n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(384, 2) #num_classes) #change from 512 -> 384\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "          # define the forward pass\n",
    "    def forward(self, embeddings):\n",
    "        # feed the embeddings through the dense layers\n",
    "\n",
    "        x = self.fc1(embeddings)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def save_weights(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    @classmethod\n",
    "    def load_weights(cls, path, sbert):\n",
    "        model = cls(sbert)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained SBERT to our defined architecture\n",
    "model = SBERT_Arch(sbert)\n",
    "\n",
    "# push the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(train_labels), y= train_labels)\n",
    "\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "# push to GPU\n",
    "weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights)\n",
    "\n",
    "# number of training epochs\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train():\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to GPU\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        embeddings, labels = batch  # Extract sentence embeddings and labels\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get model predictions for the current batch\n",
    "        preds = model(embeddings)\n",
    "\n",
    "        # Compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # Add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # Backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "\n",
    "        # Append the model predictions\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    # Compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # Reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    # Returns the loss and predictions\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_rounded)))\n",
    "\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for evaluating the model\n",
    "def evaluate():\n",
    "\n",
    "    print(\"\\nEvaluating...\")\n",
    "\n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save the model predictions\n",
    "    total_preds = []\n",
    "\n",
    "    # iterate over batches\n",
    "    for step, batch in enumerate(val_dataloader):\n",
    "\n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        # push the batch to GPU\n",
    "        batch = [t.to(device) for t in batch]\n",
    "\n",
    "        embeddings, labels = batch  # Extract sentence embeddings and labels\n",
    "\n",
    "        # Deactivate autograd\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Convert the embeddings to a list of sentences\n",
    "            #sentences = [dataset['title'][i] for i in batch[0]] #added change\n",
    "            #sentences = [dataset['title'][int(i)] for i in batch[0]]\n",
    "\n",
    "            # Model predictions\n",
    "            preds = model(embeddings)\n",
    "            #preds = model(sentences)\n",
    "\n",
    "            # Compute the validation loss between actual and predicted values\n",
    "            loss = cross_entropy(preds, labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    # Compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "\n",
    "    # Reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fine-Tuning Process\n",
    "<a name=\"fine-tuning-process\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving best model\n",
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "\n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'master_saved_weights_SAP_0410.pt')\n",
    "\n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the weights\n",
    "#path = '/content/drive/My Drive/Colab Notebooks/master_saved_weights_SAP_0410.pt'\n",
    "#torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Post-Processing\n",
    "<a name=\"post\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Embeddings Generation\n",
    "<a name=\"emb-gen\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved weights\n",
    "model_path = '/content/drive/My Drive/Colab Notebooks/master_saved_weights_SAP_0410.pt'\n",
    "saved_weights = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = dataset['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the titles to get embeddings\n",
    "finetuned_embeddings = sbert.encode(titles, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `finetuned_embeddings` is a tensor\n",
    "finetuned_embeddings_list = finetuned_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Embeddings Averaging over Date\n",
    "<a name=\"emb-avg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['finetuned_embeddings'] = finetuned_embeddings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that 'datetime' is a datetime type for proper grouping\n",
    "dataset['datetime'] = pd.to_datetime(dataset['datetime'])\n",
    "\n",
    "# Function to average lists of embeddings\n",
    "def average_embeddings(embedding_lists):\n",
    "    # Convert list of lists to a numpy array\n",
    "    embedding_array = np.array(embedding_lists)\n",
    "    # Calculate the mean along the rows\n",
    "    mean_embeddings = np.mean(embedding_array, axis=0)\n",
    "    return mean_embeddings.tolist()  # Return as list if preferred\n",
    "\n",
    "# Group by 'datetime' and aggregate using the custom function\n",
    "averaged_embeddings = dataset.groupby('datetime')['finetuned_embeddings'].agg(average_embeddings).reset_index()\n",
    "\n",
    "# Rename columns to reflect the content\n",
    "averaged_embeddings.columns = ['datetime', 'finetuned_embeddings_date']\n",
    "\n",
    "# Merge this back with the original dataset to associate each original row with the averaged embeddings of its date\n",
    "dataset = dataset.merge(averaged_embeddings, on='datetime', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list in 'finetuned_embeddings_date' to a string format for hashable operations\n",
    "dataset['finetuned_embeddings_date'] = dataset['finetuned_embeddings_date'].apply(lambda x: str(x))\n",
    "\n",
    "# Create a new DataFrame with only 'datetime' and 'finetuned_embeddings_date' columns, now with strings\n",
    "finetuning_output_SAP = dataset[['datetime', 'finetuned_embeddings_date']].drop_duplicates()\n",
    "\n",
    "# Set 'datetime' as the index and rename the index\n",
    "finetuning_output_SAP.set_index('datetime', inplace=True)\n",
    "finetuning_output_SAP.index.name = 'date'\n",
    "\n",
    "# Display the new DataFrame\n",
    "finetuning_output_SAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "finetuning_output_SAP.to_csv('/content/drive/My Drive/Colab Notebooks/finetuning_output_SAP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterenv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

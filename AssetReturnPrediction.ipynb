{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python notebook file which will contain the asset return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed \n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting seed for reproducability\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general hyperparameter for lstm\n",
    "input_steps = 20\n",
    "output_steps = 1 #one day ahead prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function - creating two sequence out of which one predicts the other (from: https://github.com/krishnaik06/Time-Series-Forecasting/blob/master/UnivariateTimeSeries.ipynb)\n",
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the loaded DataFrames\n",
    "data_dict = {}\n",
    "\n",
    "# Define the directory where the CSV files are saved\n",
    "directory = '/Users/cemakkus/PycharmProjects/Master/data/'\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('export_') and filename.endswith('.csv'):\n",
    "        # Extract the ticker from the filename\n",
    "        ticker = filename.replace('export_', '').replace('.csv', '')\n",
    "        \n",
    "        # Define the full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        data_dict[ticker] = df.iloc[output_steps:]  # Select all rows except the last rows that contain nan's\n",
    "        \n",
    "        \n",
    "        print(f'DataFrame for {ticker} loaded from {file_path}')\n",
    "\n",
    "\n",
    "# At this point, data_dict contains all DataFrames, keyed and sorted by ticker\n",
    "data_dict = {k: data_dict[k] for k in sorted(data_dict)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter for feature engineering (at this part of the code, so models use exactly the same dates for their predictions)\n",
    "n_features = 2\n",
    "window = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering for other top 3 german components in eurostoxx50 prediction\n",
    "rolmean_SAP = data_dict['SAP_DE']['1d_return'].rolling(window).mean()\n",
    "rolstd_SAP = data_dict['SAP_DE']['1d_return'].rolling(window).std()\n",
    "\n",
    "rolmean_SIE = data_dict['SIE_DE']['1d_return'].rolling(window).mean()\n",
    "rolstd_SIE = data_dict['SIE_DE']['1d_return'].rolling(window).std()\n",
    "\n",
    "rolmean_DTE = data_dict['DTE_DE']['1d_return'].rolling(window).mean()\n",
    "rolstd_DTE = data_dict['DTE_DE']['1d_return'].rolling(window).std()\n",
    "\n",
    "#adding features as columns in dataframe for dax\n",
    "data_dict['SAP_DE']['Rolling_Mean'] = rolmean_SAP\n",
    "data_dict['SAP_DE']['Rolling_Std'] = rolstd_SAP\n",
    "\n",
    "data_dict['SIE_DE']['Rolling_Mean'] = rolmean_SIE\n",
    "data_dict['SIE_DE']['Rolling_Std'] = rolstd_SIE\n",
    "\n",
    "#adding features as columns in dataframe for dax\n",
    "data_dict['DTE_DE']['Rolling_Mean'] = rolmean_DTE\n",
    "data_dict['DTE_DE']['Rolling_Std'] = rolstd_DTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#disregarding rows for which feature values can not be generated - top 3 german components\n",
    "data_dict['SAP_DE'] = data_dict['SAP_DE'][window-1:]\n",
    "\n",
    "data_dict['SIE_DE'] = data_dict['SIE_DE'][window-1:]\n",
    "\n",
    "data_dict['DTE_DE'] = data_dict['DTE_DE'][window-1:]\n",
    "\n",
    "#do this for all if needed later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pre-Processing for ARIMA (Determining Best ARIMA models & Generating Train/Test Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter for train/test split\n",
    "train_test_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split sap asset returns into train and test set\n",
    "train_arima_SAP, test_arima_SAP = train_test_split(data_dict['SAP_DE']['1d_return'], test_size=1-train_test_ratio, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determining most suitable arima order for dax prediction\n",
    "autoarima_SAP = auto_arima(train_arima_SAP, \n",
    "                      start_p=0, start_q=0,  #minimum p and q\n",
    "                      test='adf',            #use augmented dickey-fuller test to find optimal 'd'\n",
    "                      max_p=3, max_q=3,      #maximum p and q\n",
    "                      d=None,                #let model determine 'd'\n",
    "                      seasonal=False,        #no seasonality\n",
    "                      trace=True,\n",
    "                      error_action='ignore',  \n",
    "                      suppress_warnings=True, \n",
    "                      stepwise=True)\n",
    "print(autoarima_SAP.summary())\n",
    "autoarima_SAP.plot_diagnostics(figsize=(11,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pre-Processing for LSTM (Splitting Sequences & Generating Train/Test Sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general hyperparameter\n",
    "input_steps = 20\n",
    "output_steps = 1 #one day ahead prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating two sequence out of which one predicts the other - asset return of dax\n",
    "X_SAP,Y_SAP = split_sequence(data_dict['SAP_DE']['1d_return'], input_steps, output_steps)\n",
    "\n",
    "#creating two sequence out of which one predicts the other - features of SAP\n",
    "X1_SAP,Y1_SAP = split_sequence(data_dict['SAP_DE']['Rolling_Mean'], input_steps, output_steps)\n",
    "\n",
    "X2_SAP,Y2_SAP = split_sequence(data_dict['SAP_DE']['Rolling_Std'], input_steps, output_steps)\n",
    "\n",
    "#concatenating input variables for lstm\n",
    "X_ft_SAP = np.concatenate([X_SAP,X1_SAP,X2_SAP])\n",
    "\n",
    "#reshaping\n",
    "#X_SAP = np.reshape(X_SAP, (X_SAP.shape[0], input_steps, 1), order='F')\n",
    "X_ft_SAP = np.reshape(X_ft_SAP, (int(X_ft_SAP.shape[0]/(n_features+1)), input_steps, n_features + 1), order='F')\n",
    "Y_SAP = np.reshape(Y_SAP, (Y_SAP.shape[0], output_steps), order='F')\n",
    "\n",
    "#bringing Y in array form\n",
    "Y_SAP = [i[output_steps - 1] for i in Y_SAP.tolist()]\n",
    "Y_SAP = np.array(Y_SAP)\n",
    "\n",
    "#splitting sequence into train, val and test data (60-20-20 split)\n",
    "# First split: Separate out the test set\n",
    "X_temp, X_ft_test_SAP, Y_temp, Y_test_SAP = train_test_split(X_ft_SAP, Y_SAP, test_size=1-train_test_ratio, shuffle=False)\n",
    "# Second split: Split the remaining data into training and validation sets\n",
    "X_ft_train_SAP, X_ft_val_SAP, Y_train_SAP, Y_val_SAP = train_test_split(X_temp, Y_temp, test_size=0.25, shuffle=False)  # 0.25 * 0.8 = 0.2 # 0.1765 * 0.85 = 0.15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterenv_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
